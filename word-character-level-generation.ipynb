{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2124,"sourceType":"datasetVersion","datasetId":1028},{"sourceId":9301236,"sourceType":"datasetVersion","datasetId":5631836},{"sourceId":106134,"sourceType":"modelInstanceVersion","modelInstanceId":88928,"modelId":113141}],"dockerImageVersionId":30761,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Word/Character Level Generation Using LSTM\n\nHii Folks...\n\nIn This Notebook We are going to work on Character And Word Level Generation. Basically I Recently learn About LSTM And I want to create something using Them And I Found That Word Or Character Level Generation is such an Excellend Project.\n\nSo Before The Project Let's talk about few things-\n\n* **What are RNN's?**\n* **Why RNN networks Fail?**\n* **LSTM**\n\n\n\n### What Are RNN's \nHuman's Don't start there thinking From Scratch every Second.\nFor Example-> *\"He Like going to School.\"*\nYou are not able to guess What I Am Talking About? *\"Who's He\"*.\nRight But If I Give you Statement Like *\"Alex is good in acaedmics. He Like going to school\"*.\nYou Can Understand I Am talking about *Alex* Earlier.\n\nRecurrent Neural Networks address this issue. They are networks with loops in them, allowing information to persist.\n\n![Rnn Image](https://media.geeksforgeeks.org/wp-content/uploads/20231204125839/What-is-Recurrent-Neural-Network-660.webp)\n\nThey are Like Networks with loop. If they have a information Say A Sentence *\"This Cat is Big.\"*. Then Our RNN is first going to take the first word *The* and then is Just Like A Normal ANN where our input is going to multiply with weighs and adding of biases and then we are going to return our output but RNN also send that output to Next ANN and Now it is Going to take two input First *\"Cat\"* from sentence and our answer for *The* and now it is going to give response based on those two words. and It is Going to Continue until the sentence end.\n\n**Note - We are also going to convert our words to vectors. As Input Layer Takes Numbers.**\n\nThis chain-like nature reveals that recurrent neural networks are intimately related to sequences and lists. They’re the natural architecture of neural network to use for such data.\nRNN work output is dependent on the earlier context that is the reason they become so famous. Many Problems can be solved using RNN's. They solved varity of problems speech recognition, language modeling, translation, image captioning… and the list goes on.  But they really are pretty amazing.\n\nThey are also Building Blocks Of **LSTM** as well.\n\nNow main Question is So Why they need LSTM??\n\n\n### Why RNN networks Fail?\n\nOne of the main reason to RNN. That they can able to connect previous info and current task. For Example. It can use previous stock market data  to help us in making a better understanding of current market data.\nBut Can They? It actually Depends.\n\nIt depends on how previous info we are looking. For example, consider a language model trying to predict the next word based on the previous ones. If we are trying to predict the last word in “Lions are  king of Jungle” we don’t need any further context – it’s pretty obvious the next word is going to be Jungle. In such cases, where the gap between the relevant information and the place that it’s needed is small, RNNs can learn to use the past information.\n\n\nBut there are also cases where we need more context. Consider trying to predict the last word in the text “His Name is Jonathan… He speak fluent French.” Recent information suggests that the next word is probably the name of a language, but if we want to narrow down which language, we need the context of Jonathan, from further back. It’s entirely possible for the gap between the relevant information and the point where it is needed to become very large.\n\nUnfortunately, as that gap grows, RNNs become unable to learn to connect the information.\n\nIt happends due to **Vanishing Gradient Problem.** \n\nThankfully, LSTMs don’t have this problem!\n\n## **LSTM**\n\nLong Term Short Memory or LSTM Are Special Kind of RNN. They are specially created to avoid long-term dependency problem. Remembering information for long periods of time is practically their default behavior, not something they struggle to learn!\n\nIn concept, an LSTM recurrent unit tries to “remember” all the past knowledge that the network is seen so far and to “forget” irrelevant data. This is done by introducing different activation function layers called “gates” for different purposes. Each LSTM recurrent unit also maintains a vector called the Internal Cell State which conceptually describes the information that was chosen to be retained by the previous LSTM recurrent unit.\n\nLSTM networks are the most commonly used variation of Recurrent Neural Networks (RNNs). The critical component of the LSTM is the memory cell and the gates (including the forget gate but also the input gate), inner contents of the memory cell are modulated by the input gates and forget gates. This allows the LSTM model to overcome the vanishing gradient properly occurs with most Recurrent Neural Network models.\n\n","metadata":{"execution":{"iopub.status.busy":"2024-09-03T12:33:23.496375Z","iopub.execute_input":"2024-09-03T12:33:23.496767Z","iopub.status.idle":"2024-09-03T12:33:39.184177Z","shell.execute_reply.started":"2024-09-03T12:33:23.496730Z","shell.execute_reply":"2024-09-03T12:33:39.183210Z"}}},{"cell_type":"markdown","source":"## Importing The Libraries","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport nltk\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nimport string\nimport random\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional, GlobalAveragePooling1D,Input\nfrom tensorflow.keras.models import Sequential\nimport re","metadata":{"execution":{"iopub.status.busy":"2024-09-03T14:39:50.121272Z","iopub.execute_input":"2024-09-03T14:39:50.122036Z","iopub.status.idle":"2024-09-03T14:39:50.127620Z","shell.execute_reply.started":"2024-09-03T14:39:50.121996Z","shell.execute_reply":"2024-09-03T14:39:50.126692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Word Level Text Generation\nFirstly I Am going to use word level text generation where our model is going to generate word For The Input Text.","metadata":{}},{"cell_type":"code","source":"strategy = tf.distribute.MirroredStrategy()","metadata":{"execution":{"iopub.status.busy":"2024-09-03T13:26:00.078303Z","iopub.execute_input":"2024-09-03T13:26:00.078860Z","iopub.status.idle":"2024-09-03T13:26:01.131085Z","shell.execute_reply.started":"2024-09-03T13:26:00.078824Z","shell.execute_reply":"2024-09-03T13:26:01.130215Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Importing Data","metadata":{}},{"cell_type":"code","source":"data=pd.read_csv(\"/kaggle/input/shakespeare-plays/Shakespeare_data.csv\")\ndata.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset=data[\"PlayerLine\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corpus=[]\nwith strategy.scope():\n    for line in dataset:\n        lowercase_line=line.lower()\n        corpus.append(lowercase_line)\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corpus[:10]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Converting Word to integers\n\nWe are going to Create Tokenizer and Going to fit on our texts. Converting all our words to an index","metadata":{}},{"cell_type":"code","source":"tokenizer=Tokenizer()\ntokenizer.fit_on_texts(corpus)\nword_to_token=tokenizer.word_index\nindex_or_word=tokenizer.index_word","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def check_dict(num):\n    for i in range(num):\n        numrand=random.randrange(0,len(index_or_word)-1)\n        print(f\"Word- {index_or_word[numrand]} and index- {numrand}\")\n    \ncheck_dict(10)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total_words=len(word_to_token)+1\ntotal_words","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_sequences=[]\n\nwith strategy.scope():\n    c=0\n    for line in corpus:\n        token_list=tokenizer.texts_to_sequences([line])[0]\n        for i in range(1,len(token_list)):\n            n_gram_token=token_list[:i+1]\n            input_sequences.append(n_gram_token)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(input_sequences)","metadata":{"execution":{"iopub.status.busy":"2024-09-03T14:42:34.486791Z","iopub.execute_input":"2024-09-03T14:42:34.487185Z","iopub.status.idle":"2024-09-03T14:42:34.527037Z","shell.execute_reply.started":"2024-09-03T14:42:34.487150Z","shell.execute_reply":"2024-09-03T14:42:34.525728Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"before=input_sequences[0]\nbefore","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_seq_len=max(len(x) for x in input_sequences)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_seq_len","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"All the texts Should be of same length. So We are going to do predadding or adding zero. So each line going to be of same length.\n","metadata":{}},{"cell_type":"code","source":"from tensorflow.keras.preprocessing.sequence import pad_sequences\n\ninput_sequences_array = np.array(pad_sequences(input_sequences,\n                                              maxlen=max_seq_len, \n                                              padding = 'pre'))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"features=input_sequences_array[:,:-1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels=input_sequences_array[:,-1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels=tf.keras.utils.to_categorical(labels,num_classes=total_words)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels[0]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with strategy.scope():\n    n = 0.5# We are only taking a chunk of this huge dataset to fit it on the RAM\n    slice_size = int(len(features)*n)\n    np.save('/kaggle/working/features', features[:slice_size, :])\n    np.save('/kaggle/working/labels', labels[:slice_size, :])\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with strategy.scope():\n    features = np.load('/kaggle/working/features_new.npy')\n    labels = np.load('/kaggle/working/labels_new.npy')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"total_words","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Creating Dataset for word level text Generation","metadata":{}},{"cell_type":"code","source":"dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n\n# Optional: Shuffle and batch the dataset\nBATCH_SIZE = 32\ndataset = dataset.shuffle(buffer_size=len(features)).batch(BATCH_SIZE)\n\n# Verify the dataset\nfor batch_features, batch_labels in dataset.take(1):\n    print(\"Batch Features Shape:\", batch_features.shape)\n    print(\"Batch Labels Shape:\", batch_labels.shape)\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Creating Model ","metadata":{}},{"cell_type":"code","source":"def generate_model():\n    tf.random.set_seed(42)\n    model = Sequential()\n    model.add(Embedding(total_words, 100, input_length=max_seq_len-1))  # Corrected\n    model.add(Bidirectional(LSTM(64, return_sequences=True)))            # Corrected\n    model.add(Bidirectional(LSTM(32)))                                   # Corrected\n    model.add(Dense(64, activation='relu'))                              # Corrected\n    model.add(Dense(total_words, activation='softmax'))                  # Corrected\n    return model\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with strategy.scope():\n    model = generate_model()\n    model.compile(loss=\"categorical_crossentropy\",\n                  optimizer=tf.keras.optimizers.Adam(learning_rate=0.002),\n                  metrics=['accuracy'])\n    model.build(input_shape=(None, max_seq_len-1))\n    model.summary()\n    ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"EPOCHS = 50\nhistory = model.fit(features, labels, epochs = EPOCHS)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_graph(history, string):\n    plt.plot(history.history[string])\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(string)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-09-03T15:42:24.204829Z","iopub.execute_input":"2024-09-03T15:42:24.205156Z","iopub.status.idle":"2024-09-03T15:42:24.210204Z","shell.execute_reply.started":"2024-09-03T15:42:24.205123Z","shell.execute_reply":"2024-09-03T15:42:24.209326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plot_graph(history, 'accuracy')\nplot_graph(history, 'loss')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Saving The Word Level Model","metadata":{}},{"cell_type":"code","source":"model.save('/kaggle/working/test_generator_model.h5')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Testing Our Model","metadata":{}},{"cell_type":"code","source":"def test_generator(string, num):\n    if len(string)==0:\n        print(\"Error: No word found\")\n        return\n    for _ in range(num):\n        token_list = tokenizer.texts_to_sequences([string])[0]\n        token_list = pad_sequences([token_list], maxlen=max_seq_len-1, padding = \"pre\")\n        probabilities = model.predict(token_list)\n        choice = np.random.choice([1,2,3])\n        predicted = np.argsort(probabilities, axis = -1)[0][-choice]\n        if predicted !=0:\n            generated_word = tokenizer.index_word[predicted]\n            string += \" \" + generated_word\n    print(string)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"max_seq_len","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_generator(\"Enter KING HENRY\", 20)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Character Level Text Generation\n\nWe are going to Create A Model Which is Going to generate *characters.* ","metadata":{}},{"cell_type":"markdown","source":"For this I Am Going to use *nietzsche works*","metadata":{}},{"cell_type":"code","source":"!curl -O https://s3.amazonaws.com/text-datasets/nietzsche.txt","metadata":{"execution":{"iopub.status.busy":"2024-09-03T13:26:01.132316Z","iopub.execute_input":"2024-09-03T13:26:01.132930Z","iopub.status.idle":"2024-09-03T13:26:02.802700Z","shell.execute_reply.started":"2024-09-03T13:26:01.132884Z","shell.execute_reply":"2024-09-03T13:26:02.801727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open(\"/kaggle/working/nietzsche.txt\",\"r\") as file:\n    text=file.read()","metadata":{"execution":{"iopub.status.busy":"2024-09-03T13:26:02.805466Z","iopub.execute_input":"2024-09-03T13:26:02.806317Z","iopub.status.idle":"2024-09-03T13:26:02.811874Z","shell.execute_reply.started":"2024-09-03T13:26:02.806263Z","shell.execute_reply":"2024-09-03T13:26:02.810828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Creating Dataset","metadata":{}},{"cell_type":"code","source":"text[:10]","metadata":{"execution":{"iopub.status.busy":"2024-09-03T13:26:02.812952Z","iopub.execute_input":"2024-09-03T13:26:02.813208Z","iopub.status.idle":"2024-09-03T13:26:02.826470Z","shell.execute_reply.started":"2024-09-03T13:26:02.813180Z","shell.execute_reply":"2024-09-03T13:26:02.825580Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 64\nraw_data_ds = tf.data.TextLineDataset([\"nietzsche.txt\"])","metadata":{"execution":{"iopub.status.busy":"2024-09-03T13:26:16.117709Z","iopub.execute_input":"2024-09-03T13:26:16.118109Z","iopub.status.idle":"2024-09-03T13:26:16.152625Z","shell.execute_reply.started":"2024-09-03T13:26:16.118076Z","shell.execute_reply":"2024-09-03T13:26:16.151717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for elem in raw_data_ds.take(10):\n    print(elem.numpy().decode(\"utf-8\"))","metadata":{"execution":{"iopub.status.busy":"2024-09-03T13:26:16.404174Z","iopub.execute_input":"2024-09-03T13:26:16.404507Z","iopub.status.idle":"2024-09-03T13:26:16.460337Z","shell.execute_reply.started":"2024-09-03T13:26:16.404474Z","shell.execute_reply":"2024-09-03T13:26:16.459449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text=\"\"\n\nfor elem in raw_data_ds:\n    text+=elem.numpy().decode('utf-8')","metadata":{"execution":{"iopub.status.busy":"2024-09-03T13:26:16.799741Z","iopub.execute_input":"2024-09-03T13:26:16.800527Z","iopub.status.idle":"2024-09-03T13:26:17.953748Z","shell.execute_reply.started":"2024-09-03T13:26:16.800479Z","shell.execute_reply":"2024-09-03T13:26:17.952898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(text)\n","metadata":{"execution":{"iopub.status.busy":"2024-09-03T13:26:17.955388Z","iopub.execute_input":"2024-09-03T13:26:17.955739Z","iopub.status.idle":"2024-09-03T13:26:17.961733Z","shell.execute_reply.started":"2024-09-03T13:26:17.955702Z","shell.execute_reply":"2024-09-03T13:26:17.960724Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"chars=sorted(list(set(text)))\nprint(\"Total disctinct chars:\", len(chars))\n","metadata":{"execution":{"iopub.status.busy":"2024-09-03T13:26:17.962850Z","iopub.execute_input":"2024-09-03T13:26:17.963138Z","iopub.status.idle":"2024-09-03T13:26:17.991864Z","shell.execute_reply.started":"2024-09-03T13:26:17.963108Z","shell.execute_reply":"2024-09-03T13:26:17.990909Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"maxlen = 20\nstep = 3\ninput_chars = [] # X\nnext_char = []   # y","metadata":{"execution":{"iopub.status.busy":"2024-09-03T13:26:18.521010Z","iopub.execute_input":"2024-09-03T13:26:18.521286Z","iopub.status.idle":"2024-09-03T13:26:18.527410Z","shell.execute_reply.started":"2024-09-03T13:26:18.521257Z","shell.execute_reply":"2024-09-03T13:26:18.526640Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(0,len(text)-maxlen,step):\n    input_chars.append(text[i:i+maxlen])\n    next_char.append(text[i + maxlen])","metadata":{"execution":{"iopub.status.busy":"2024-09-03T13:26:19.074496Z","iopub.execute_input":"2024-09-03T13:26:19.074806Z","iopub.status.idle":"2024-09-03T13:26:19.208140Z","shell.execute_reply.started":"2024-09-03T13:26:19.074775Z","shell.execute_reply":"2024-09-03T13:26:19.207374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_chars[1],next_char[1]","metadata":{"execution":{"iopub.status.busy":"2024-09-03T13:26:19.764871Z","iopub.execute_input":"2024-09-03T13:26:19.765156Z","iopub.status.idle":"2024-09-03T13:26:19.771114Z","shell.execute_reply.started":"2024-09-03T13:26:19.765127Z","shell.execute_reply":"2024-09-03T13:26:19.770132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Number of sequences:\", len(input_chars))\nprint(\"input X  (input_chars)  --->   output y (next_char) \")\nfor i in range(8):\n      print( input_chars[i],\"   --->  \", next_char[i])","metadata":{"execution":{"iopub.status.busy":"2024-09-03T13:26:20.616614Z","iopub.execute_input":"2024-09-03T13:26:20.616937Z","iopub.status.idle":"2024-09-03T13:26:20.622456Z","shell.execute_reply.started":"2024-09-03T13:26:20.616906Z","shell.execute_reply":"2024-09-03T13:26:20.621590Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Using Tensorflow Slices","metadata":{}},{"cell_type":"code","source":"X_train_ds_raw=tf.data.Dataset.from_tensor_slices(input_chars)\ny_train_ds_raw=tf.data.Dataset.from_tensor_slices(next_char)","metadata":{"execution":{"iopub.status.busy":"2024-09-03T13:26:21.407476Z","iopub.execute_input":"2024-09-03T13:26:21.407804Z","iopub.status.idle":"2024-09-03T13:26:22.604827Z","shell.execute_reply.started":"2024-09-03T13:26:21.407772Z","shell.execute_reply":"2024-09-03T13:26:22.603815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for elem1,elem2 in zip(X_train_ds_raw.take(5),y_train_ds_raw.take(5)):\n    print(elem1.numpy().decode('utf-8'),\"----->\", elem2.numpy().decode('utf-8'))","metadata":{"execution":{"iopub.status.busy":"2024-09-03T13:26:22.606367Z","iopub.execute_input":"2024-09-03T13:26:22.606704Z","iopub.status.idle":"2024-09-03T13:26:22.629701Z","shell.execute_reply.started":"2024-09-03T13:26:22.606671Z","shell.execute_reply":"2024-09-03T13:26:22.628901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Model constants.\nmax_features = 83              # Number of distinct chars  \nembedding_dim = 16             # Embedding layer output dimension\nsequence_length = maxlen ","metadata":{"execution":{"iopub.status.busy":"2024-09-03T13:26:22.630766Z","iopub.execute_input":"2024-09-03T13:26:22.631050Z","iopub.status.idle":"2024-09-03T13:26:22.635469Z","shell.execute_reply.started":"2024-09-03T13:26:22.631020Z","shell.execute_reply":"2024-09-03T13:26:22.634566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Text Preprocessing","metadata":{}},{"cell_type":"code","source":"def custom_standardization(input_data):\n    lowercase     = tf.strings.lower(input_data)\n    stripped_html = tf.strings.regex_replace(lowercase, \"<br />\", \" \")\n    stripped_num  = tf.strings.regex_replace(stripped_html, \"[\\d-]\", \" \")\n    stripped_punc  =tf.strings.regex_replace(stripped_num, \n                             \"[%s]\" % re.escape(string.punctuation), \"\")    \n    return stripped_punc\n\ndef char_split(input_data):\n    \n    return tf.strings.unicode_split(input_data, 'UTF-8')","metadata":{"execution":{"iopub.status.busy":"2024-09-03T13:26:23.543425Z","iopub.execute_input":"2024-09-03T13:26:23.543808Z","iopub.status.idle":"2024-09-03T13:26:23.549904Z","shell.execute_reply.started":"2024-09-03T13:26:23.543773Z","shell.execute_reply":"2024-09-03T13:26:23.548989Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Creating Text Vectorization Layer","metadata":{}},{"cell_type":"code","source":"vectorize_layer = tf.keras.layers.TextVectorization(\n    standardize=custom_standardization,      # Preprocessing function\n    max_tokens= max_features,                # Maximum size of the vocabulary for this layer     \n    split=char_split,                        # Split into chars\n    output_mode=\"int\",                       # convert to int\n    output_sequence_length=sequence_length,  # Training sample length\n)","metadata":{"execution":{"iopub.status.busy":"2024-09-03T13:26:24.017903Z","iopub.execute_input":"2024-09-03T13:26:24.018192Z","iopub.status.idle":"2024-09-03T13:26:24.033880Z","shell.execute_reply.started":"2024-09-03T13:26:24.018161Z","shell.execute_reply":"2024-09-03T13:26:24.033071Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vectorize_layer.adapt(X_train_ds_raw.batch(batch_size))","metadata":{"execution":{"iopub.status.busy":"2024-09-03T13:26:24.633366Z","iopub.execute_input":"2024-09-03T13:26:24.633676Z","iopub.status.idle":"2024-09-03T13:26:40.380826Z","shell.execute_reply.started":"2024-09-03T13:26:24.633632Z","shell.execute_reply":"2024-09-03T13:26:40.379783Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"The first 10 entries: \", vectorize_layer.get_vocabulary()[:10])","metadata":{"execution":{"iopub.status.busy":"2024-09-03T13:26:40.382451Z","iopub.execute_input":"2024-09-03T13:26:40.382790Z","iopub.status.idle":"2024-09-03T13:26:40.389703Z","shell.execute_reply.started":"2024-09-03T13:26:40.382755Z","shell.execute_reply":"2024-09-03T13:26:40.388824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in X_train_ds_raw.take(2):\n    print(i.numpy().decode(\"utf-8\"))","metadata":{"execution":{"iopub.status.busy":"2024-09-03T13:26:40.393285Z","iopub.execute_input":"2024-09-03T13:26:40.393710Z","iopub.status.idle":"2024-09-03T13:26:40.408521Z","shell.execute_reply.started":"2024-09-03T13:26:40.393643Z","shell.execute_reply":"2024-09-03T13:26:40.407692Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def vectorize_text(text):\n    text = tf.expand_dims(text, -1)\n    return tf.squeeze(vectorize_layer(text))","metadata":{"execution":{"iopub.status.busy":"2024-09-03T13:26:40.410380Z","iopub.execute_input":"2024-09-03T13:26:40.410675Z","iopub.status.idle":"2024-09-03T13:26:40.415303Z","shell.execute_reply.started":"2024-09-03T13:26:40.410628Z","shell.execute_reply":"2024-09-03T13:26:40.414432Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train_ds = X_train_ds_raw.map(vectorize_text)\ny_train_ds = y_train_ds_raw.map(vectorize_text)\n\nX_train_ds.element_spec, y_train_ds.element_spec","metadata":{"execution":{"iopub.status.busy":"2024-09-03T13:26:40.416395Z","iopub.execute_input":"2024-09-03T13:26:40.416708Z","iopub.status.idle":"2024-09-03T13:26:40.650452Z","shell.execute_reply.started":"2024-09-03T13:26:40.416672Z","shell.execute_reply":"2024-09-03T13:26:40.649508Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for text in X_train_ds.take(1):\n    print(text.numpy())\nfor text in y_train_ds.take(1):\n    print(text.numpy())","metadata":{"execution":{"iopub.status.busy":"2024-09-03T13:26:40.651639Z","iopub.execute_input":"2024-09-03T13:26:40.651954Z","iopub.status.idle":"2024-09-03T13:26:40.760309Z","shell.execute_reply.started":"2024-09-03T13:26:40.651922Z","shell.execute_reply":"2024-09-03T13:26:40.759320Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_train_ds=y_train_ds.map(lambda x: x[0])","metadata":{"execution":{"iopub.status.busy":"2024-09-03T13:26:40.761943Z","iopub.execute_input":"2024-09-03T13:26:40.762307Z","iopub.status.idle":"2024-09-03T13:26:40.787928Z","shell.execute_reply.started":"2024-09-03T13:26:40.762266Z","shell.execute_reply":"2024-09-03T13:26:40.787015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for elem in y_train_ds.take(1):\n    print(\"shape: \", elem.shape, \"\\n next_char: \",elem.numpy())","metadata":{"execution":{"iopub.status.busy":"2024-09-03T13:26:47.179116Z","iopub.execute_input":"2024-09-03T13:26:47.179836Z","iopub.status.idle":"2024-09-03T13:26:47.236289Z","shell.execute_reply.started":"2024-09-03T13:26:47.179795Z","shell.execute_reply":"2024-09-03T13:26:47.235470Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_ds =  tf.data.Dataset.zip((X_train_ds,y_train_ds))","metadata":{"execution":{"iopub.status.busy":"2024-09-03T13:26:47.464558Z","iopub.execute_input":"2024-09-03T13:26:47.464870Z","iopub.status.idle":"2024-09-03T13:26:47.473658Z","shell.execute_reply.started":"2024-09-03T13:26:47.464840Z","shell.execute_reply":"2024-09-03T13:26:47.472906Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for (X,y) in zip(X_train_ds.take(5), y_train_ds.take(5)):\n    print(X.numpy(),\" --> \",y.numpy())","metadata":{"execution":{"iopub.status.busy":"2024-09-03T13:26:47.732427Z","iopub.execute_input":"2024-09-03T13:26:47.732737Z","iopub.status.idle":"2024-09-03T13:26:47.832429Z","shell.execute_reply.started":"2024-09-03T13:26:47.732705Z","shell.execute_reply":"2024-09-03T13:26:47.831529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"AUTOTUNE = tf.data.AUTOTUNE\ntrain_ds = train_ds.shuffle(buffer_size=512).batch(batch_size, drop_remainder=True).cache().prefetch(buffer_size=AUTOTUNE)","metadata":{"execution":{"iopub.status.busy":"2024-09-03T13:26:48.063456Z","iopub.execute_input":"2024-09-03T13:26:48.063792Z","iopub.status.idle":"2024-09-03T13:26:48.076899Z","shell.execute_reply.started":"2024-09-03T13:26:48.063760Z","shell.execute_reply":"2024-09-03T13:26:48.076123Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for sample in train_ds.take(1):\n    print(\"input (X) dimension: \", sample[0].numpy().shape, \"\\noutput (y) dimension: \",sample[1].numpy().shape)","metadata":{"execution":{"iopub.status.busy":"2024-09-03T13:26:48.513186Z","iopub.execute_input":"2024-09-03T13:26:48.513465Z","iopub.status.idle":"2024-09-03T13:26:48.690823Z","shell.execute_reply.started":"2024-09-03T13:26:48.513436Z","shell.execute_reply":"2024-09-03T13:26:48.689928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for sample in train_ds.take(1):\n  print(\"input (sequence of chars): \", sample[0][0].numpy(), \"\\noutput (next char to complete the input): \",sample[1][0].numpy())","metadata":{"execution":{"iopub.status.busy":"2024-09-03T13:26:48.943759Z","iopub.execute_input":"2024-09-03T13:26:48.944119Z","iopub.status.idle":"2024-09-03T13:26:49.125370Z","shell.execute_reply.started":"2024-09-03T13:26:48.944083Z","shell.execute_reply":"2024-09-03T13:26:49.124459Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def decode_sequence(encoded_sequence):\n    \n    deceoded_sequence=[]\n    for token in encoded_sequence:\n        deceoded_sequence.append(vectorize_layer.get_vocabulary()[token])\n    sequence= ''.join(deceoded_sequence)\n    print(\"\\n\",sequence,\"\\n\")\n    return sequence","metadata":{"execution":{"iopub.status.busy":"2024-09-03T14:23:00.740618Z","iopub.execute_input":"2024-09-03T14:23:00.741048Z","iopub.status.idle":"2024-09-03T14:23:00.746628Z","shell.execute_reply.started":"2024-09-03T14:23:00.741012Z","shell.execute_reply":"2024-09-03T14:23:00.745702Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for sample in train_ds.take(7):\n     print(\"input (sequence of chars): \", \n           decode_sequence (sample[0][0].numpy()), \n           \"\\noutput (next char to complete the input): \",\n           vectorize_layer.get_vocabulary()[sample[1][0].numpy()]\n          )","metadata":{"execution":{"iopub.status.busy":"2024-09-03T14:23:01.656209Z","iopub.execute_input":"2024-09-03T14:23:01.656874Z","iopub.status.idle":"2024-09-03T14:23:01.812227Z","shell.execute_reply.started":"2024-09-03T14:23:01.656836Z","shell.execute_reply":"2024-09-03T14:23:01.811306Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def temperature_sampling (conditional_probability, temperature=1.0):\n    conditional_probability = np.asarray(conditional_probability).astype(\"float64\")\n    conditional_probability = np.log(conditional_probability) / temperature\n    reweighted_conditional_probability = softmax(conditional_probability)\n    probas = np.random.multinomial(1, reweighted_conditional_probability, 1)\n    return np.argmax(probas)","metadata":{"execution":{"iopub.status.busy":"2024-09-03T13:26:50.437122Z","iopub.execute_input":"2024-09-03T13:26:50.437420Z","iopub.status.idle":"2024-09-03T13:26:50.442446Z","shell.execute_reply.started":"2024-09-03T13:26:50.437388Z","shell.execute_reply":"2024-09-03T13:26:50.441570Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def top_k_sampling(conditional_probability, k):\n    top_k_probabilities, top_k_indices= tf.math.top_k(conditional_probability, k=k, sorted=True)\n    top_k_probabilities= np.asarray(top_k_probabilities).astype(\"float32\")\n    top_k_probabilities= np.squeeze(top_k_probabilities)\n    top_k_indices = np.asarray(top_k_indices).astype(\"int32\")\n    top_k_redistributed_probability=softmax(top_k_probabilities)\n    top_k_redistributed_probability = np.asarray(top_k_redistributed_probability).astype(\"float32\")\n    sampled_token = np.random.choice(np.squeeze(top_k_indices), p=top_k_redistributed_probability)\n    return sampled_token","metadata":{"execution":{"iopub.status.busy":"2024-09-03T13:26:51.238977Z","iopub.execute_input":"2024-09-03T13:26:51.239280Z","iopub.status.idle":"2024-09-03T13:26:51.245544Z","shell.execute_reply.started":"2024-09-03T13:26:51.239249Z","shell.execute_reply":"2024-09-03T13:26:51.244660Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tensorflow.keras import layers, Model\nimport tensorflow.keras.backend as K\n","metadata":{"execution":{"iopub.status.busy":"2024-09-03T13:26:52.091627Z","iopub.execute_input":"2024-09-03T13:26:52.091991Z","iopub.status.idle":"2024-09-03T13:26:52.097554Z","shell.execute_reply.started":"2024-09-03T13:26:52.091956Z","shell.execute_reply":"2024-09-03T13:26:52.096505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Creating Character Level Model","metadata":{}},{"cell_type":"code","source":"inputs = tf.keras.Input(shape=(sequence_length,), dtype=\"int64\")\nx = layers.Embedding(max_features, embedding_dim)(inputs)\nx = layers.LSTM(64, return_sequences=True)(x)\nx = layers.LSTM(128, return_sequences=True)(x)\nx = layers.LSTM(128, return_sequences=False)(x)\npredictions=  layers.Dense(max_features, activation='softmax')(x)\nmodel_LSTM = tf.keras.Model(inputs, predictions,name=\"model_LSTM\")","metadata":{"execution":{"iopub.status.busy":"2024-09-03T15:10:16.123543Z","iopub.execute_input":"2024-09-03T15:10:16.124227Z","iopub.status.idle":"2024-09-03T15:10:16.196271Z","shell.execute_reply.started":"2024-09-03T15:10:16.124188Z","shell.execute_reply":"2024-09-03T15:10:16.195320Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"callback = tf.keras.callbacks.EarlyStopping(monitor=\"loss\", patience=5)\nmodel_LSTM.compile(loss='sparse_categorical_crossentropy', \n                   optimizer='adam', metrics=['accuracy'])\nprint(model_LSTM.summary())","metadata":{"execution":{"iopub.status.busy":"2024-09-03T15:10:17.405857Z","iopub.execute_input":"2024-09-03T15:10:17.406464Z","iopub.status.idle":"2024-09-03T15:10:17.431638Z","shell.execute_reply.started":"2024-09-03T15:10:17.406427Z","shell.execute_reply":"2024-09-03T15:10:17.430816Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vectorize_layer(\"Hii I Am\")","metadata":{"execution":{"iopub.status.busy":"2024-09-03T15:10:18.382470Z","iopub.execute_input":"2024-09-03T15:10:18.383620Z","iopub.status.idle":"2024-09-03T15:10:18.398847Z","shell.execute_reply.started":"2024-09-03T15:10:18.383570Z","shell.execute_reply":"2024-09-03T15:10:18.397883Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"history=model_LSTM.fit(train_ds,epochs=100,callbacks=[callback])","metadata":{"execution":{"iopub.status.busy":"2024-09-03T15:10:48.870297Z","iopub.execute_input":"2024-09-03T15:10:48.870698Z","iopub.status.idle":"2024-09-03T15:42:24.202732Z","shell.execute_reply.started":"2024-09-03T15:10:48.870661Z","shell.execute_reply":"2024-09-03T15:42:24.201369Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Plotiing Accuracy And Loss","metadata":{}},{"cell_type":"code","source":"plot_graph(history,\"accuracy\")\nplot_graph(history,\"loss\")","metadata":{"execution":{"iopub.status.busy":"2024-09-03T15:44:50.746280Z","iopub.execute_input":"2024-09-03T15:44:50.746673Z","iopub.status.idle":"2024-09-03T15:44:51.160286Z","shell.execute_reply.started":"2024-09-03T15:44:50.746625Z","shell.execute_reply":"2024-09-03T15:44:51.159358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def greedy_search(conditional_probability):\n    \n    return (np.argmax(conditional_probability))","metadata":{"execution":{"iopub.status.busy":"2024-09-03T15:44:54.835242Z","iopub.execute_input":"2024-09-03T15:44:54.836007Z","iopub.status.idle":"2024-09-03T15:44:54.840188Z","shell.execute_reply.started":"2024-09-03T15:44:54.835968Z","shell.execute_reply":"2024-09-03T15:44:54.839195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def softmax(z):\n     return np.exp(z)/sum(np.exp(z))\n    \ndef temperature_sampling (conditional_probability, temperature=1.0):\n    conditional_probability = np.asarray(conditional_probability).astype(\"float64\")\n    conditional_probability = np.log(conditional_probability) / temperature\n    reweighted_conditional_probability = softmax(conditional_probability)\n    probas = np.random.multinomial(1, reweighted_conditional_probability, 1)\n    return np.argmax(probas)","metadata":{"execution":{"iopub.status.busy":"2024-09-03T15:44:55.333744Z","iopub.execute_input":"2024-09-03T15:44:55.334162Z","iopub.status.idle":"2024-09-03T15:44:55.340788Z","shell.execute_reply.started":"2024-09-03T15:44:55.334124Z","shell.execute_reply":"2024-09-03T15:44:55.339765Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# using only the top K props as distribution\ndef top_k_sampling(conditional_probability, k):\n    top_k_probabilities, top_k_indices= tf.math.top_k(conditional_probability, k=k, sorted=True)\n    top_k_probabilities= np.asarray(top_k_probabilities).astype(\"float32\")\n    top_k_probabilities= np.squeeze(top_k_probabilities)\n    top_k_indices = np.asarray(top_k_indices).astype(\"int32\")\n    top_k_redistributed_probability=softmax(top_k_probabilities)\n    top_k_redistributed_probability = np.asarray(top_k_redistributed_probability).astype(\"float32\")\n    sampled_token = np.random.choice(np.squeeze(top_k_indices), p=top_k_redistributed_probability)\n    return sampled_token","metadata":{"execution":{"iopub.status.busy":"2024-09-03T15:44:56.844107Z","iopub.execute_input":"2024-09-03T15:44:56.844718Z","iopub.status.idle":"2024-09-03T15:44:56.850871Z","shell.execute_reply.started":"2024-09-03T15:44:56.844678Z","shell.execute_reply":"2024-09-03T15:44:56.849943Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Testing Out Our Model.\nWe are going to use three sample techniques  **Top K sampling**, **Temperature Sampling** and **Greedy Search**\nWe are going to predict our next character based on these. \n\nSo How these Algorithms Work-\n* Top K Sampling - It does exactly what it sounds it is going to selects the most k probable words or characters.\n* Greedy Search- It is Simple. It is going to select word or character with highest probability.\n* Temperature Sampling- You will set a temperature from 0 to more than 1. The More you closer to 0 the words with high  probability only get chances but when as you go towards more distant words like you set temperature of 1.5. Model Also pick random words or words with less probability","metadata":{}},{"cell_type":"markdown","source":"### Testing Our Model","metadata":{}},{"cell_type":"code","source":"def generate_text(model, seed_original, step):\n    seed= vectorize_text(seed_original)\n    print(\"The prompt is\")\n    decode_sequence(seed.numpy().squeeze())\n    \n\n    seed= vectorize_text(seed_original).numpy().reshape(1,-1)\n    #Text Generated by Greedy Search Sampling\n    generated_greedy_search = (seed)\n    for i in range(step):\n        predictions=model.predict(seed)\n        next_index= greedy_search(predictions.squeeze())\n        generated_greedy_search = np.append(generated_greedy_search, next_index)\n        seed= generated_greedy_search[-sequence_length:].reshape(1,sequence_length)\n    print(\"Text Generated by Greedy Search Sampling:\")\n    decode_sequence(generated_greedy_search)\n\n    #Text Generated by Temperature Sampling\n    print(\"Text Generated by Temperature Sampling:\")\n    for temperature in [0.2, 0.5, 1.0, 1.2]:\n        print(\"\\ttemperature: \", temperature)\n        seed= vectorize_text(seed_original).numpy().reshape(1,-1)\n        generated_temperature = (seed)\n        for i in range(step):\n            predictions=model.predict(seed)\n            next_index = temperature_sampling(predictions.squeeze(), temperature)\n            generated_temperature = np.append(generated_temperature, next_index)\n            seed= generated_temperature[-sequence_length:].reshape(1,sequence_length)\n        decode_sequence(generated_temperature)\n\n    #Text Generated by Top-K Sampling\n    print(\"Text Generated by Top-K Sampling:\")\n    for k in [2, 3, 4, 5]:\n        print(\"\\tTop-k: \", k)\n        seed= vectorize_text(seed_original).numpy().reshape(1,-1)\n        generated_top_k = (seed)\n        for i in range(step):\n            predictions=model.predict(seed)\n            next_index = top_k_sampling(predictions.squeeze(), k)\n            generated_top_k = np.append(generated_top_k, next_index)\n            seed= generated_top_k[-sequence_length:].reshape(1,sequence_length)\n        decode_sequence(generated_top_k)","metadata":{"execution":{"iopub.status.busy":"2024-09-03T15:44:59.029095Z","iopub.execute_input":"2024-09-03T15:44:59.029435Z","iopub.status.idle":"2024-09-03T15:44:59.040918Z","shell.execute_reply.started":"2024-09-03T15:44:59.029401Z","shell.execute_reply":"2024-09-03T15:44:59.040002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"generate_text(model_LSTM,\"Truth is a woma\", 1) \n","metadata":{"execution":{"iopub.status.busy":"2024-09-03T15:45:00.073622Z","iopub.execute_input":"2024-09-03T15:45:00.074426Z","iopub.status.idle":"2024-09-03T15:45:01.008343Z","shell.execute_reply.started":"2024-09-03T15:45:00.074383Z","shell.execute_reply":"2024-09-03T15:45:01.007477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"text[:10]","metadata":{"execution":{"iopub.status.busy":"2024-09-03T15:45:09.288057Z","iopub.execute_input":"2024-09-03T15:45:09.288428Z","iopub.status.idle":"2024-09-03T15:45:09.296084Z","shell.execute_reply.started":"2024-09-03T15:45:09.288391Z","shell.execute_reply":"2024-09-03T15:45:09.295201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"generate_text(model_LSTM,\"is a wo\", 5) ","metadata":{"execution":{"iopub.status.busy":"2024-09-03T15:45:09.755127Z","iopub.execute_input":"2024-09-03T15:45:09.755428Z","iopub.status.idle":"2024-09-03T15:45:12.546412Z","shell.execute_reply.started":"2024-09-03T15:45:09.755398Z","shell.execute_reply":"2024-09-03T15:45:12.545500Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Saving Our Model","metadata":{}},{"cell_type":"code","source":"model_LSTM.save(\"/kaggle/working/LSTM_Model_new.h5\")","metadata":{"execution":{"iopub.status.busy":"2024-09-03T15:45:23.253425Z","iopub.execute_input":"2024-09-03T15:45:23.254069Z","iopub.status.idle":"2024-09-03T15:45:23.299599Z","shell.execute_reply.started":"2024-09-03T15:45:23.254029Z","shell.execute_reply":"2024-09-03T15:45:23.298818Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}